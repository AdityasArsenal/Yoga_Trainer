{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNYcOeR7qFzMxvKExbBlCWn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdityasArsenal/Yoga_Trainer/blob/main/FineTuned_mobilenetv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syAEtdvmWWsE"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "m9xZVH7hW9DR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!sudo apt -qq install git-lfs\n",
        "!git config --global credential.helper store"
      ],
      "metadata": {
        "id": "Ahqd8y7DYsye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gdRoxBMJbMcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -al \"/content/drive/MyDrive/YogaPoses.zip/\""
      ],
      "metadata": {
        "id": "AOF5pf6jbrfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/YogaPoses.zip\" -d /tmp/yogaimg"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rkpFAP9tcksQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "ds = load_dataset(\"imagefolder\", data_dir=\"/tmp/yogaimg/YogaPoses\")\n",
        "ds = ds['train']"
      ],
      "metadata": {
        "id": "QzIZge4zctVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = ds.train_test_split(test_size=0.15)\n",
        "data"
      ],
      "metadata": {
        "id": "GfMQHtR8voJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.push_to_hub(\"AdityasArsenal/YogaDataSet\")"
      ],
      "metadata": {
        "id": "3cumm6aPnH0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_dataset(\"AdityasArsenal/YogaDataSet\")"
      ],
      "metadata": {
        "id": "r4t6Sfs5wnZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ex = data['train'][402]\n",
        "ex"
      ],
      "metadata": {
        "id": "SIwiwKjAs64u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = ex['image']\n",
        "image"
      ],
      "metadata": {
        "id": "wRpVvZe7tJ5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = data['train'].features['label']\n",
        "labels"
      ],
      "metadata": {
        "id": "7AAQIcSBtfMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels.int2str(ex['label'])"
      ],
      "metadata": {
        "id": "9DdTgTXOtiRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers evaluate"
      ],
      "metadata": {
        "collapsed": true,
        "id": "e7aA1-j-tnWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "metric = load(\"accuracy\")"
      ],
      "metadata": {
        "id": "COyM5R4UvOqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "svW88hjuw7ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = data[\"train\"].features[\"label\"].names\n",
        "label2id, id2label = dict(), dict()\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = i\n",
        "    id2label[i] = label"
      ],
      "metadata": {
        "id": "u_abbJXcxDaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTFeatureExtractor\n",
        "\n",
        "model_name_or_path = 'google/mobilenet_v2_1.0_224'\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name_or_path)"
      ],
      "metadata": {
        "id": "okscg6dzxGAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import (\n",
        "    CenterCrop,\n",
        "    Compose,\n",
        "    Normalize,\n",
        "    RandomHorizontalFlip,\n",
        "    RandomResizedCrop,\n",
        "    Resize,\n",
        "    ToTensor,\n",
        ")\n",
        "\n",
        "# MobileNetV2 normalization values\n",
        "normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "# Training transformations\n",
        "train_transforms = Compose(\n",
        "    [\n",
        "        RandomResizedCrop(224),  # Resize to 224x224 for MobileNetV2\n",
        "        RandomHorizontalFlip(),\n",
        "        ToTensor(),\n",
        "        normalize,\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Validation transformations\n",
        "val_transforms = Compose(\n",
        "    [\n",
        "        Resize(224),  # Resize to 224x224 for MobileNetV2\n",
        "        CenterCrop(224),\n",
        "        ToTensor(),\n",
        "        normalize,\n",
        "    ]\n",
        ")\n",
        "\n",
        "def preprocess_train(example_batch):\n",
        "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
        "    example_batch[\"pixel_values\"] = [\n",
        "        train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]\n",
        "    ]\n",
        "    return example_batch\n",
        "\n",
        "def preprocess_val(example_batch):\n",
        "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
        "    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
        "    return example_batch\n"
      ],
      "metadata": {
        "id": "zq6LlAso2Btm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split up training into training + validation\n",
        "train_ds = data['train']\n",
        "val_ds = data['test']"
      ],
      "metadata": {
        "id": "FBHuH40D3oTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.map(preprocess_train, batched = True)\n",
        "val_ds = val_ds.map(preprocess_val, batched = True)"
      ],
      "metadata": {
        "id": "8FFmshoR45jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForImageClassification\n",
        "from torch import nn\n",
        "\n",
        "# Model name for MobileNetV2\n",
        "model_name_or_path = 'google/mobilenet_v2_1.0_224'\n",
        "\n",
        "# Your labels (yoga poses)\n",
        "labels = ['Downdog', 'Goddess', 'Plank', 'Tree', 'Warrior2']\n",
        "\n",
        "# Load the model with ignore_mismatched_sizes=True to skip layer size mismatches\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    num_labels=len(labels),  # Number of classes in your dataset (5)\n",
        "    id2label={str(i): label for i, label in enumerate(labels)},  # Mapping indices to class names\n",
        "    label2id={label: str(i) for i, label in enumerate(labels)},  # Mapping class names to indices\n",
        "    ignore_mismatched_sizes=True  # Ignore size mismatch for the classifier layers\n",
        ")\n",
        "\n",
        "# Access the classifier layer and modify it\n",
        "in_features = model.classifier.in_features  # Get the input features for the classifier\n",
        "\n",
        "# Replace the classifier with a new one\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(0.2),  # Optional dropout layer\n",
        "    nn.Linear(in_features, len(labels))  # Update the output to the correct number of classes\n",
        ")\n",
        "\n",
        "# Now the model is ready to be fine-tuned for your dataset\n"
      ],
      "metadata": {
        "id": "Hc-9Azpi8yBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='finetuned-for-YogaPoses',  # Output directory for the fine-tuned model\n",
        "    per_device_train_batch_size=16,  # You can adjust the batch size depending on GPU memory\n",
        "    eval_strategy=\"steps\",  # Use eval_strategy instead of evaluation_strategy\n",
        "    num_train_epochs=4,  # Number of training epochs\n",
        "    fp16=True,  # Use mixed precision if your hardware supports it (e.g., GPUs with tensor cores)\n",
        "    save_steps=100,  # Save model every 100 steps\n",
        "    eval_steps=100,  # Evaluate every 100 steps\n",
        "    logging_steps=10,  # Log metrics every 10 steps\n",
        "    learning_rate=2e-4,  # Learning rate for training\n",
        "    save_total_limit=2,  # Only keep the last two checkpoints\n",
        "    remove_unused_columns=False,  # Do not remove unused columns from the dataset\n",
        "    push_to_hub=True,  # Push the fine-tuned model to Hugging Face Hub\n",
        "    report_to='tensorboard',  # Use TensorBoard for logging\n",
        "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
        "    hub_strategy=\"end\",  # Push the model to the hub at the end of training\n",
        "    metric_for_best_model='accuracy',  # Optional: Specify the metric to use for selecting the best model\n",
        ")\n"
      ],
      "metadata": {
        "id": "o-0Q39LjIw-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# The compute_metrics function takes a NamedTuple as input:\n",
        "# predictions, which are the logits of the model as Numpy arrays,\n",
        "# and label_ids, which are the ground-truth labels as Numpy arrays.\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
        "    # Get the predicted labels by applying argmax on the logits\n",
        "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
        "\n",
        "    # Compute the accuracy using sklearn's accuracy_score\n",
        "    accuracy = accuracy_score(eval_pred.label_ids, predictions)\n",
        "\n",
        "    return {'accuracy': accuracy}\n"
      ],
      "metadata": {
        "id": "J69Z2GzPJh7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
        "        'labels': torch.tensor([x['label'] for x in batch])\n",
        "    }"
      ],
      "metadata": {
        "id": "LBiLTXLXJt-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, AutoFeatureExtractor\n",
        "\n",
        "# Load the feature extractor\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained('google/mobilenet_v2_1.0_224')\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,  # Your MobileNetV2 model\n",
        "    args=training_args,  # Your training arguments\n",
        "    train_dataset=train_ds,  # Training dataset\n",
        "    eval_dataset=val_ds,  # Validation dataset\n",
        "    compute_metrics=compute_metrics,  # Compute accuracy\n",
        "    data_collator=collate_fn,  # Data collator for batching\n",
        ")\n"
      ],
      "metadata": {
        "id": "340oIw3xJ2BG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_results = trainer.train()\n",
        "# rest is optional but nice to have\n",
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_state()"
      ],
      "metadata": {
        "id": "SJisNSYuJ9Tf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = trainer.evaluate()\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "trainer.save_metrics(\"eval\", metrics)"
      ],
      "metadata": {
        "id": "Kl4xLDtLKkgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kwargs = {\n",
        "    \"finetuned_from\": model.config._name_or_path,\n",
        "    \"tasks\": \"image-classification\",\n",
        "    \"dataset\": 'indian_food_images',\n",
        "    \"tags\": ['image-classification'],\n",
        "}\n",
        "\n",
        "if training_args.push_to_hub:\n",
        "    trainer.push_to_hub('🍻 cheers', **kwargs)\n",
        "else:\n",
        "    trainer.create_model_card(**kwargs)"
      ],
      "metadata": {
        "id": "Z-u3-6hCWluZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n",
        "from PIL import Image\n",
        "import torch\n",
        "import requests\n",
        "import io\n"
      ],
      "metadata": {
        "id": "Zxz2JxmfaKyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide the path to your uploaded image\n",
        "image_path = \"/content/pp.jpg\"\n",
        "\n",
        "# Open the image\n",
        "image = Image.open(image_path)\n"
      ],
      "metadata": {
        "id": "VdzNRslLaONg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image"
      ],
      "metadata": {
        "id": "oLn1ixHzabs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the feature extractor for MobileNetV2\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained('google/mobilenet_v2_1.0_224')\n",
        "\n",
        "# Preprocess the image\n",
        "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n"
      ],
      "metadata": {
        "id": "1cnJrT3OajyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with the trained model\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():  # Disable gradient calculation\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Get the predicted class (max logit)\n",
        "predicted_class = torch.argmax(outputs.logits, dim=-1).item()\n",
        "\n",
        "# Print the predicted class\n",
        "print(f\"Predicted class index: {predicted_class}\")\n",
        "print(f\"Predicted class label: {model.config.id2label[str(predicted_class)]}\")\n"
      ],
      "metadata": {
        "id": "aU_eORn4aqep"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}